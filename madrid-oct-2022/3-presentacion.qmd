---
format: revealjs
execute: 
  freeze: true
  eval: false
---

```{r}
#| include: false
library(sparklyr)
library(dplyr)

sc <- spark_connect("local")

spark_mtcars <- copy_to(sc, mtcars)
spark_iris <- copy_to(sc, iris)
```


## Spark en R con `sparklyr`

<br><br>

### Edgar Ruiz

RStudio PBC (Posit)

<br><br>

::: columns
::: column
:::

::: column

{{< fa brands github >}} edgararuiz <br>
{{< fa brands linkedin >}} /in/edgararuiz <br>
{{< fa brands twitter >}} @theotheredgar <br> 

:::
:::

## Apache Spark

::: columns

::: column 
::: {.incremental}
-   Análysis de datos a escala
-   Librería extensiva de algoritmos de ML
-   Interactúa con SQL
-   Cálculos se pueden distribuir en multiples computadoras
-   Grátis, y "open source"
:::
:::

::: column
![](imagenes/spark.svg)
:::
:::

## `sparklyr`

::: columns

::: column
::: {.incremental}
-   Provée acceso a Spark por medio de R
-   Permite el uso de las interfazes de `DBI`, `dplyr`, `broom`
-   Facilita la creación de "ML Pipelines"
-   Posible correr código R dentro de Spark
:::
:::


::: column
![](imagenes/sparklyr.png)
:::
:::

## Filosofía de `sparklyr`


:::{.incremental}
- Con la excepción de `spark_apply()`, las funciones de `sparklyr` fueron creadas
para activar la ejecución de operaciónes de Spark. 

- Por ejemplo, para crear un modelo de regresión linear, `sparklyr` no corre la 
función `lm()` dentro de Spark. 

- `sparklyr` utiliza la función `ml_linear_regression()`,
que utiliza el algoritmo de modelo linear que estan dentro de las librerias
de Spark.
:::
## `dplyr` para manipulación de datos

Podemos utilizar las mismas operaciones de `dplyr` que utilizamos en nuestro
trabajo diario para manipular datos dentro de Spark.

:::: {.columns}
::: {.column}
```{r}
#| echo: true
mtcars %>% 
  filter(mpg > 20) %>% 
  select(mpg, am) %>% 
  head(5)
```
:::

::: {.column}
```{r}
#| echo: true
tbl(sc, "mtcars") %>% 
  filter(mpg > 20) %>% 
  select(mpg, am) %>% 
  head(5)
```
:::
::::

## `dplyr` traduce para nosotros

`dplyr` puede "traducir" los pasos de conversión de datos a SQL que Spark entiende.

:::{.incremental}
:::: {.columns}
::: {.column width="40%"}
```{r}
#| echo: true
tbl(sc, "mtcars") %>% 
  filter(mpg > 20) %>% 
  select(mpg, am) %>% 
  head(5) %>% 
  show_query()
```
:::

::: {.column}
- **Tip!** -- Aprender `dplyr`, ahora tiene la gran ventaja de que proveé interfaces a otras technologías, como bases de datos (`dbplyr`), Arrow (`arrow`), y (`data.table`)
:::
::::
:::

## Recursos - Sitio oficial

spark.rstudio.com

![](imagenes/sitio.png)


## Recursos 

::::{.columns}
:::{.column width="30%"}
Libro

![](imagenes/book.png)
:::

:::{.column}

Cheatsheet
![](imagenes/cheatsheet.png)
:::
::::

## Las últimas noticias... `r emo::ji("newspaper")`

<br><br>

::: {.incremental}

- Funciones nuevas <br><br>
- Artículos nuevos

:::

## Medidas a-la `yardstick`

```{r}
#| include: false
iris_split <- sdf_random_split(spark_iris, training = 0.5, test = 0.5)
reg_formula <- "Sepal_Length ~ Sepal_Width + Petal_Length + Petal_Width"
model <- ml_generalized_linear_regression(iris_split$training, reg_formula)
tbl_predictions <- ml_predict(model, iris_split$test)
```

Nuevas funciones devuelve los resultados en un `tibble` al estilo `metrics()`
del paquete `yardstick`
  
- `ml_metrics_binary()`

- `ml_metrics_multiclass()`

- `ml_metrics_regression()`
  
```{r}
#| echo: true
tbl_predictions %>%
  ml_metrics_regression(Sepal_Length)
```

## Articulos

::::{.columns}
:::{.column}
:::{.incremental}
- [Modelando datos de texto](https://spark.rstudio.com/guides/textmodeling.html)

  - [Palabras en español para remover](https://github.com/apache/spark/blob/0494dc90af48ce7da0625485a4dc6917a244d580/mllib/src/main/resources/org/apache/spark/ml/feature/stopwords/spanish.txt)
  
- [Afinando modelos](https://spark.rstudio.com/guides/model_tuning.html)

- [Afinando modelos usando combinaciones de parametros](https://spark.rstudio.com/guides/model_tuning_text.html)
:::
:::

:::{.column}
:::
::::
```{r}
#| include: false
spark_disconnect(sc)
```

