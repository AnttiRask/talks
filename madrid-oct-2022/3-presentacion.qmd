---
format: revealjs
execute: 
  freeze: true
  eval: true
---

```{r}
#| include: false
library(sparklyr)
library(dplyr)

sc <- spark_connect("local")

spark_mtcars <- copy_to(sc, mtcars)
spark_iris <- copy_to(sc, iris)
```


## Spark en R con `sparklyr`

<br><br>

### Edgar Ruiz

RStudio PBC (Posit)

<br><br>

::: columns
::: column
:::

::: column

{{< fa brands github >}} edgararuiz <br>
{{< fa brands linkedin >}} /in/edgararuiz <br>
{{< fa brands twitter >}} @theotheredgar <br> 

:::
:::

##  Introducción {background-color="#333"}

:::{.incremental}
- Spark
- `sparklyr`
:::

## Apache Spark

::: columns

::: column 
::: {.incremental}
-   Análysis de datos a escala
-   Librería extensiva de algoritmos de ML
-   Interactúa con SQL
-   Cálculos se pueden distribuir en multiples computadoras
-   Grátis, y "open source"
:::
:::

::: column
![](imagenes/spark.svg)
:::
:::

## ¿Que quiere decir?

Podemos analyzar datos a **gran** escala. Los datos y el procesamiento se distribuyen
a múltiples computadoras.

```{mermaid}

classDiagram
  class Servidor1 {
    Maneja el Proceso
  }
  class Servidor2{
    Datos\n\nProceso
  }
  class Servidor3{
    Datos\n\nProceso     
  }
  class Servidor4{
    Datos\n\nProceso  
  }  
  class Servidor5{
    Datos\n\nProceso  
  }    
  Servidor1 --> Servidor2
  Servidor1 --> Servidor3
  Servidor1 --> Servidor4
  Servidor1 --> Servidor5

```


## `sparklyr`

::: columns

::: column
::: {.incremental}
-   Provée acceso a Spark por medio de R
-   Permite el uso de las interfazes de `DBI`, `dplyr`, `broom`
-   Facilita la creación de "ML Pipelines"
-   Posible correr código R dentro de Spark
:::
:::


::: column
![](imagenes/sparklyr.png)
:::
:::

## Filosofía de `sparklyr`


:::{.incremental}
- Con la excepción de `spark_apply()`, las funciones de `sparklyr` fueron creadas
para activar la ejecución de operaciónes de Spark. 

- Por ejemplo, para crear un modelo de regresión linear, `sparklyr` no corre la 
función `lm()` dentro de Spark. 

- `sparklyr` utiliza la función `ml_linear_regression()`,
que utiliza el algoritmo de modelo linear que estan dentro de las librerias
de Spark.
:::

## Enfoque de `sparklyr`

`sparklyr` nos permite concentrarnos en explorar los datos, utilizando los mismos
pasos que utilizamos para anlyzar datos "locales" en nuestra computadora.


![](imagenes/data-science-explore.svg) 



Libro ["R para Ciencia de Datos" - Wickham & Grolemund](https://es.r4ds.hadley.nz/)

## `dplyr` para manipulación de datos

Podemos utilizar las mismas operaciones de `dplyr` que utilizamos en nuestro
trabajo diario para manipular datos dentro de Spark.

:::: {.columns}
::: {.column}
```{r}
#| echo: true
mtcars %>% 
  filter(mpg > 20) %>% 
  select(mpg, am) %>% 
  head(5)
```
:::

::: {.column}
```{r}
#| echo: true
tbl(sc, "mtcars") %>% 
  filter(mpg > 20) %>% 
  select(mpg, am) %>% 
  head(5)
```
:::
::::

## `dplyr` traduce para nosotros

`dplyr` puede "traducir" los pasos de conversión de datos a SQL que Spark entiende.

:::{.incremental}
:::: {.columns}
::: {.column width="40%"}
```{r}
#| echo: true
tbl(sc, "mtcars") %>% 
  filter(mpg > 20) %>% 
  select(mpg, am) %>% 
  head(5) %>% 
  show_query()
```
:::

::: {.column}
- **Tip!** -- Aprender `dplyr`, ahora tiene la gran ventaja de que proveé interfaces a otras technologías, como bases de datos (`dbplyr`), Arrow (`arrow`), y `data.table` (`dtplyr`)
:::
::::
:::

## ¿Necesito un "cluster" para aprender?


**¡No!** Por medio de `sparklyr`, podemos instalar y correr Spark en nuestra
computadora

```r
spark_install()
sc <- spark_connect("local")
```

![](imagenes/jobs.png)

##  Recursos {background-color="#333"}

## Sitio oficial

spark.rstudio.com

![](imagenes/sitio.png)


## Otros 

::::{.columns}
:::{.column width="30%"}
Libro

![](imagenes/book.png)
:::

:::{.column}

Cheatsheet
![](imagenes/cheatsheet.png)
:::
::::

##  Puntos aleatorios... {background-color="#333"}

## Arrow con Spark

Recientemente, archivos tipo *Parquet* han tomado popularidad. La nueva tecnología
llamada Arrow, acelera la lectura de estos archivos. El paquete `arrow` 
permite agregar esta tecnología en nuestra sesión de Spark.

<br>

::::{.columns}

:::{.column}
```r
library(arrow)
library(sparklyr)

sc <- spark_connect("...")
x <- spark_read_parquet("folder")
```
:::

:::{.column}
![](imagenes/arrow.png)
:::

::::

## Utilize "ML Pipelines"

Spark introduce el concepto de entrenar, y ejecutar conversiones por medio de
"tuberías". La ídea es que cada sección de el "tubo" ejecuta conversiones de 
datos, incluyendo modelaje. 

![](imagenes/ml-Pipeline.png) 

## Pipelines en `sparklyr`

::::{.columns}
:::{.column width="40%"}
```r
ml_pipeline(sc) %>% 
  ft_tokenizer(
    input_col = "review",
    output_col = "word_list"
  ) %>% 
  ft_stop_words_remover(
    input_col = "word_list", 
    output_col = "wo_stop_words"
    ) %>% 
  ft_hashing_tf(
    input_col = "wo_stop_words", 
    output_col = "hashed_features", 
    binary = TRUE, 
    num_features = 1024
    ) %>%
  ft_normalizer(
    input_col = "hashed_features", 
    output_col = "normal_features"
    ) %>% 
  ft_r_formula(score ~ normal_features) %>% 
  ml_logistic_regression() 
```
:::

:::{.column width="60%"}
```r
#> Pipeline (Estimator) with 6 stages
#> <pipeline__87caaa39_2fa9_4708_a1e1_20ab570c8917> 
#>   Stages 
#>   |--1 Tokenizer (Transformer)
#>   |    <tokenizer__e3cf3ba6_f7e9_4a05_a41d_11963d70fd6c> 
#>   |     (Parameters -- Column Names)
#>   |      input_col: review
#>   |      output_col: word_list
#>   |--2 StopWordsRemover (Transformer)
#>   |    <stop_words_remover__3fc0bf48_9fa0_441a_9bb3_5a19ec72be0f> 
#>   |     (Parameters -- Column Names)
#>   |      input_col: word_list
#>   |      output_col: wo_stop_words
#>   |--3 HashingTF (Transformer)
#>   |    <hashing_tf__3fa3d087_39e8_4668_9921_28150a53412c> 
#>   |     (Parameters -- Column Names)
#>   |      input_col: wo_stop_words
#>   |      output_col: hashed_features
#>   |--4 Normalizer (Transformer)
#>   |    <normalizer__6d4d9c1c_7488_4a4d_8d42_d9830de4ee2f> 
#>   |     (Parameters -- Column Names)
#>   |      input_col: hashed_features
#>   |      output_col: normal_features
#>   |--5 RFormula (Estimator)
#>   |    <r_formula__4ae7b190_ce59_4d5f_b75b_cbd623e1a790> 
#>   |     (Parameters -- Column Names)
#>   |      features_col: features
#>   |      label_col: label
#>   |     (Parameters)
#>   |      force_index_label: FALSE
#>   |      formula: score ~ normal_features
#>   |      handle_invalid: error
#>   |      stringIndexerOrderType: frequencyDesc
#>   |--6 LogisticRegression (Estimator)
#>   |    <logistic_regression__46c6e5fb_7c70_44f2_a366_f0a7f94801e1> 
#>   |     (Parameters -- Column Names)
#>   |      features_col: features
#>   |      label_col: label
#>   |      prediction_col: prediction
#>   |      probability_col: probability
#>   |      raw_prediction_col: rawPrediction
#>   |     (Parameters)
#>   |      aggregation_depth: 2
#>   |      elastic_net_param: 0
#>   |      family: auto
#>   |      fit_intercept: TRUE
#>   |      max_iter: 100
#>   |      maxBlockSizeInMB: 0
#>   |      reg_param: 0
#>   |      standardization: TRUE
#>   |      threshold: 0.5
#>   |      tol: 1e-06
```
:::
::::

## Las últimas noticias... `r emo::ji("newspaper")` {background-color="#333"}

<br><br>

::: {.incremental}

- Funciones nuevas <br><br>
- Artículos nuevos

:::

## Medidas a-la `yardstick`

```{r}
#| include: false
iris_split <- sdf_random_split(spark_iris, training = 0.5, test = 0.5)
reg_formula <- "Sepal_Length ~ Sepal_Width + Petal_Length + Petal_Width"
model <- ml_generalized_linear_regression(iris_split$training, reg_formula)
tbl_predictions <- ml_predict(model, iris_split$test)
```

Nuevas funciones devuelve los resultados en un `tibble` al estilo `metrics()`
del paquete `yardstick`
  
- `ml_metrics_binary()`

- `ml_metrics_multiclass()`

- `ml_metrics_regression()`
  
```{r}
#| echo: true
tbl_predictions %>%
  ml_metrics_regression(Sepal_Length)
```

## Artículos

::::{.columns}
:::{.column}
:::{.incremental}
- [Modelando datos de texto](https://spark.rstudio.com/guides/textmodeling.html)

  - [Palabras en español para remover](https://github.com/apache/spark/blob/0494dc90af48ce7da0625485a4dc6917a244d580/mllib/src/main/resources/org/apache/spark/ml/feature/stopwords/spanish.txt)
  
- [Afinando modelos](https://spark.rstudio.com/guides/model_tuning.html)

- [Afinando modelos usando combinaciones de parámetros](https://spark.rstudio.com/guides/model_tuning_text.html)
:::
:::

:::{.column}
:::
::::
```{r}
#| include: false
spark_disconnect(sc)
```

