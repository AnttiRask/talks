---
format: revealjs
execute: 
  freeze: true
  eval: false
---

```{r}
#| include: false
library(sparklyr)
library(dplyr)

sc <- spark_connect("local")

spark_mtcars <- copy_to(sc, mtcars)
spark_iris <- copy_to(sc, iris)
```


## Spark en R con `sparklyr`

<br><br>

### Edgar Ruiz

RStudio PBC (Posit)

<br><br>

::: columns
::: column
:::

::: column

{{< fa brands github >}} edgararuiz <br>
{{< fa brands linkedin >}} /in/edgararuiz <br>
{{< fa brands twitter >}} @theotheredgar <br> 

:::
:::

## Apache Spark

::: columns

::: column 
::: {.incremental}
-   Análysis de datos a escala
-   Librería extensiva de algoritmos de ML
-   Interactúa con SQL
-   Cálculos se pueden distribuir en multiples computadoras
-   Grátis, y "open source"
:::
:::

::: column
![](imagenes/spark.svg)
:::
:::

## ¿Que quiere decir?

Podemos analyzar datos a **gran** escala. Los datos y el procesamiento se distribuyen
a múltiples computadoras.

```{mermaid}

classDiagram
  class Servidor1 {
    Maneja el Proceso
  }
  class Servidor2{
    Datos\n\nProceso
  }
  class Servidor3{
    Datos\n\nProceso     
  }
  class Servidor4{
    Datos\n\nProceso  
  }  
  class Servidor5{
    Datos\n\nProceso  
  }    
  Servidor1 --> Servidor2
  Servidor1 --> Servidor3
  Servidor1 --> Servidor4
  Servidor1 --> Servidor5

```


## `sparklyr`

::: columns

::: column
::: {.incremental}
-   Provée acceso a Spark por medio de R
-   Permite el uso de las interfazes de `DBI`, `dplyr`, `broom`
-   Facilita la creación de "ML Pipelines"
-   Posible correr código R dentro de Spark
:::
:::


::: column
![](imagenes/sparklyr.png)
:::
:::

## Filosofía de `sparklyr`


:::{.incremental}
- Con la excepción de `spark_apply()`, las funciones de `sparklyr` fueron creadas
para activar la ejecución de operaciónes de Spark. 

- Por ejemplo, para crear un modelo de regresión linear, `sparklyr` no corre la 
función `lm()` dentro de Spark. 

- `sparklyr` utiliza la función `ml_linear_regression()`,
que utiliza el algoritmo de modelo linear que estan dentro de las librerias
de Spark.
:::

## Enfoque de `sparklyr`

`sparklyr` nos permite concentrarnos en explorar los datos, utilizando los mismos
pasos que utilizamos para anlyzar datos "locales" en nuestra computadora.


![](imagenes/data-science-explore.svg) 



Libro ["R para Ciencia de Datos" - Wickham & Grolemund](https://es.r4ds.hadley.nz/)

## `dplyr` para manipulación de datos

Podemos utilizar las mismas operaciones de `dplyr` que utilizamos en nuestro
trabajo diario para manipular datos dentro de Spark.

:::: {.columns}
::: {.column}
```{r}
#| echo: true
mtcars %>% 
  filter(mpg > 20) %>% 
  select(mpg, am) %>% 
  head(5)
```
:::

::: {.column}
```{r}
#| echo: true
tbl(sc, "mtcars") %>% 
  filter(mpg > 20) %>% 
  select(mpg, am) %>% 
  head(5)
```
:::
::::

## `dplyr` traduce para nosotros

`dplyr` puede "traducir" los pasos de conversión de datos a SQL que Spark entiende.

:::{.incremental}
:::: {.columns}
::: {.column width="40%"}
```{r}
#| echo: true
tbl(sc, "mtcars") %>% 
  filter(mpg > 20) %>% 
  select(mpg, am) %>% 
  head(5) %>% 
  show_query()
```
:::

::: {.column}
- **Tip!** -- Aprender `dplyr`, ahora tiene la gran ventaja de que proveé interfaces a otras technologías, como bases de datos (`dbplyr`), Arrow (`arrow`), y `data.table` (`dtplyr`)
:::
::::
:::

## ¿Necesito un "cluster" para aprender?


**¡No!** Por medio de `sparklyr`, podemos instalar y correr Spark en nuestra
computadora

```r
spark_install()
sc <- spark_connect("local")
```

![](imagenes/jobs.png)

##  Recursos {background-color="#333"}

## Sitio oficial

spark.rstudio.com

![](imagenes/sitio.png)


## Otros 

::::{.columns}
:::{.column width="30%"}
Libro

![](imagenes/book.png)
:::

:::{.column}

Cheatsheet
![](imagenes/cheatsheet.png)
:::
::::

##  Recomendaciones {background-color="#333"}

## Arrow con Spark

Recientemente, archivos tipo *Parquet* han tomado popularidad. La nueva tecnología
llamada Arrow, acelera la lectura de estos archivos. El paquete `arrow` 
permite agregar esta tecnología en nuestra sesión de Spark.

<br>

::::{.columns}

:::{.column}
```r
library(arrow)
library(sparklyr)

sc <- spark_connect("...")
x <- spark_read_parquet("folder")
```
:::

:::{.column}
![](imagenes/arrow.png)
:::

::::


## Las últimas noticias... `r emo::ji("newspaper")` {background-color="#333"}

<br><br>

::: {.incremental}

- Funciones nuevas <br><br>
- Artículos nuevos

:::

## Medidas a-la `yardstick`

```{r}
#| include: false
iris_split <- sdf_random_split(spark_iris, training = 0.5, test = 0.5)
reg_formula <- "Sepal_Length ~ Sepal_Width + Petal_Length + Petal_Width"
model <- ml_generalized_linear_regression(iris_split$training, reg_formula)
tbl_predictions <- ml_predict(model, iris_split$test)
```

Nuevas funciones devuelve los resultados en un `tibble` al estilo `metrics()`
del paquete `yardstick`
  
- `ml_metrics_binary()`

- `ml_metrics_multiclass()`

- `ml_metrics_regression()`
  
```{r}
#| echo: true
tbl_predictions %>%
  ml_metrics_regression(Sepal_Length)
```

## Artículos

::::{.columns}
:::{.column}
:::{.incremental}
- [Modelando datos de texto](https://spark.rstudio.com/guides/textmodeling.html)

  - [Palabras en español para remover](https://github.com/apache/spark/blob/0494dc90af48ce7da0625485a4dc6917a244d580/mllib/src/main/resources/org/apache/spark/ml/feature/stopwords/spanish.txt)
  
- [Afinando modelos](https://spark.rstudio.com/guides/model_tuning.html)

- [Afinando modelos usando combinaciones de parámetros](https://spark.rstudio.com/guides/model_tuning_text.html)
:::
:::

:::{.column}
:::
::::
```{r}
#| include: false
spark_disconnect(sc)
```

